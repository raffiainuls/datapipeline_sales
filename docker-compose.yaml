version: '3'

x-spark-common: &spark-common
  image: bitnami/spark:latest
  volumes:
      - ./spark-apps:/opt/bitnami/spark/jobs
      - ./jars_spark:/opt/bitnami/spark/jars

x-airflow-common: &airflow-common
  build: 
      context: .
      dockerfile: airflow_image/dockerfile
  env_file:
    - airflow.env
  environment:
      - JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
  volumes:
    - ./spark-apps:/opt/airflow/jobs
    - ./airflow/dags:/opt/airflow/dags
    - ./airflow/logs:/opt/airflow/logs
    - ./jars_spark:/opt/spark/jars
  depends_on:
    - postgres

services:
  postgres:
    image: postgres:13
    container_name: postgres
    hostname: postgres
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
    volumes:
      - ./postgres-db-volume:/var/lib/postgresql/data
    ports:
      - 5433:5432
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "postgres"]
      interval: 60s
      retries: 5
      start_period: 5s
    restart: always
    command:
      - postgres
      - -c
      - wal_level=logical
  
  mysql:
    image: mysql:5.7  
    container_name: mysql
    ports: 
      - 3306:3306
    environment:
      - MYSQL_ROOT_PASSWORD=root
      - MYSQL_USER=mysql
      - MYSQL_PASSWORD=pass
      - MYSQL_DATABASE=salesdb
    restart: always
    command: 
      - --server-id=1          # put unique id to server mysql in cluster/binlog replication 
      - --log-bin=mysql-bin     # active binary logging (binlog) and save into mysql-bin
      - --binlog-format=ROW     # format binlog 
      - --binlog-row-image=FULL
      - --gtid_mode=ON
      - --enforce-gtid-consistency=ON
      # - --log_replica_updates=ON
    volumes:
      - ./mysql_data:/var/lib/mysql

  sqlserver:
    image: mcr.microsoft.com/mssql/server:2022-latest
    container_name: sqlserver
    ports:
      - "1433:1433"
    restart: always
    environment:
      - ACCEPT_EULA=Y
      - MSSQL_PID=Developer
      - SA_PASSWORD=Sql@123456
    volumes:
      - sqlserver_data:/var/opt/mssql


  # cassandra:
  #   image: cassandra:latest
  #   container_name: cassandra 
  #   ports:
  #     - 9042:9042
  #   environment:
  #     - CASSANDRA_CLUSTER_NAME=KafkaCluster 
  #     - CASSANDRA_NUM_TOKENS=256
  #     - CASSANDRA_START_RPC=true 
  #     - CASSANDRA_ENDPOINT_SNITCH=GossipingPropertyFileSnitch
  #   volumes:
  #     - ./cassandra_data:/var/lib/cassandra 
    
  # studio:
  #   image: datastax/dse-studio
  #   ports:
  #     - 9091:9091
  #   depends_on:
  #     - cassandra
  #   environment:
  #     DS_LICENSE: accept


  mongodb:
    # image: mongo:6.0
    build: .
    container_name: mongodb
    ports:
      - "27017:27017"
    command: >
      mongod --replSet rs0
             --auth
             --keyFile /etc/mongo-keyfile
    environment:
      - MONGO_INITDB_ROOT_USERNAME=root
      - MONGO_INITDB_ROOT_PASSWORD=rootpass
    volumes:
      - mongodb_data:/data/db
      # - ./mongo-keyfile:/etc/mongo-keyfile:ro


  mongo-init:
    image: mongo:6.0
    depends_on:
      - mongodb
    entrypoint: ["bash", "-c"]
    command: >
      "
      sleep 10 &&
      mongosh --host mongodb:27017 -u root -p root --authenticationDatabase admin --eval '
        rs.initiate({
          _id: \"rs0\",
          members: [{ _id: 0, host: \"host.docker.internal:27017\" }]
        })'
      "
  # oracle:
  #   image: 'container-registry.oracle.com/database/express:latest'
  #   container_name: oracle
  #   hostname: oracle
  #   ports:
  #     - 1522:1521
  #   environment:
  #     - ORACLE_PWD=Oracle123
  #     - ORACLE_CHARACTERSET=AL32UTF8
  #   volumes:
  #     - ./oracle_data:/opt/oracle/oradata  # bind mount: local_folder : container_folder

  oracle:
    image: gvenzl/oracle-xe:21-slim
    container_name: oracle
    restart: unless-stopped
    ports:
      - "1521:1521"     # ✅ Default Oracle SQL*Net port for Debezium (corrected from 1512)
      - "5500:5500"     # Oracle EM Express (optional, can be disabled)
    environment:
      ORACLE_PASSWORD: Oracle123         # SYS, SYSTEM, PDBADMIN password
      APP_USER: appuser                  # ✅ Required to match with APP_USER_PASSWORD
      APP_USER_PASSWORD: app123          # ✅ Password for appuser
      ORACLE_DATABASE: salesdb           # ✅ PDB name (Debezium will connect here)
      ENABLE_ARCHIVELOG: true            # ✅ Required for LogMiner CDC
      ORACLE_CHARACTERSET: AL32UTF8
    volumes:
      - ./oracle-data:/opt/oracle/oradata
  
  control-center:
    hostname: control-center 
    image: confluentinc/cp-enterprise-control-center:latest
    container_name: control-center 
    depends_on:
      connect:
        condition: service_healthy
    ports:
      - 9021:9021
    environment:
      CONTROL_CENTER_BOOTSTRAP_SERVERS: "host.docker.internal:9093"
      CONTROL_CENTER_ZOOKEEPER_CONNECT: "host.docker.internal:2181"
      # CONTROL_CENTER_KSQL_KSQLDB1_URL: "http://ksqldb-server:8088"
      # CONTROL_CENTER_KSQL_ENABLE: "true"
      CONTROL_CENTER_SCHEMA_REGISTRY_URL: "http://schema-registry:8081"
      # CONTROL_CENTER_KSQL_KSQLDB1_URL: "http://ksqldb-server:8088"
      # CONTROL_CENTER_KSQL_KSQLDB1_ADVERTISED_URL: "http://localhost:8088"
      # CONTROL_CENTER_KSQL_KSQLDB1: "ksqlDB Cluster"
      CONTROL_CENTER_CONNECT_CLUSTER: "http://connect:8083"
      CONTROL_CENTER_INTERNAL_TOPICS_PARTITIONS: 1
      CONTROL_CENTER_INTERNAL_TOPICS_REPLICATION: 1
      CONTROL_CENTER_MONITORING_INTERCEPTOR_TOPIC_PARTITIONS: 1
      CONTROL_CENTER_MONITORING_INTERCEPTOR_TOPIC_REPLICATION: 1
      CONTROL_CENTER_METRICS_TOPIC_REPLICATION: 1
      CONTROL_CENTER_CONNECT_METRICS_TOPIC_REPLICATION: 1
      # CONTROL_CENTER_KSQLDB_TOPIC_REPLICATION: 1
      CONTROL_CENTER_REPLICATION_FACTOR: 1
      CONTROL_CENTER_STREAMS_CACHE_MAX_BYTES_BUFFERING: 10485760
      CONTROL_CENTER_STREAMS_COMMIT_INTERVAL_MS: 5000
      CONTROL_CENTER_STREAMS_NUM_STREAM_THREADS: 2
      CONTROL_CENTER_STREAMS_CONSUMER_REQUEST_TIMEOUT_MS: 30000
      
  connect:
    image: confluentinc/cp-kafka-connect:latest
    container_name: connect
    # depends_on:
    #   broker:
    #     condition: service_healthy
    ports:
      - "8083:8083"
    environment:
      CONNECT_BOOTSTRAP_SERVERS: "host.docker.internal:9093"
      CONNECT_REST_ADVERTISED_HOST_NAME: connect
      CONNECT_REST_PORT: 8083
      CONNECT_GROUP_ID: connect
      CONNECT_CONFIG_STORAGE_TOPIC: docker-connect-configs
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_OFFSET_FLUSH_INTERVAL_MS: 10000
      CONNECT_OFFSET_STORAGE_TOPIC: docker-connect-offsets
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_STATUS_STORAGE_TOPIC: docker-connect-status
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_KEY_CONVERTER: org.apache.kafka.connect.storage.StringConverter
      # CONNECT_VALUE_CONVERTER: org.apache.kafka.connect.storage.StringConverter
      CONNECT_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter 
      # CLASSPATH: /usr/share/java/monitoring-interceptors/monitoring-interceptors-6.0.1.jar
      # CONNECT_PRODUCER_INTERCEPTOR_CLASSES: io.confluent.monitoring.clients.interceptor.MonitoringProducerInterceptor
      # CONNECT_CONSUMER_INTERCEPTOR_CLASSES: io.confluent.monitoring.clients.interceptor.MonitoringConsumerInterceptor
      CONNECT_PLUGIN_PATH: /usr/share/java,/usr/share/confluent-hub-components,/usr/share/java/new-connector
      CONNECT_LOG4J_LOGGERS: org.apache.zookeeper=ERROR,org.I0Itec.zkclient=ERROR,org.reflections=ERROR
    volumes:
      - ./lib:/usr/share/java/new-connector
    #   - ./connector-confluent-hub:/usr/share/confluent-hub-components
    command:
      - bash
      - -c
      - >+
        echo "Installing Connector"

        confluent-hub install --no-prompt debezium/debezium-connector-mysql:latest

        confluent-hub install --no-prompt debezium/debezium-connector-postgresql:2.4.2

        confluent-hub install --no-prompt debezium/debezium-connector-sqlserver:2.4.2

        confluent-hub install --no-prompt debezium/debezium-connector-mongodb:latest

        confluent-hub install --no-prompt confluentinc/kafka-connect-jdbc:10.7.6

        confluent-hub install --no-prompt confluentinc/kafka-connect-oracle-cdc:latest

        confluent-hub install --no-prompt confluentinc/monitoring-interceptors:latest

        confluent-hub install --no-prompt debezium/debezium-connector-oracle:latest

        confluent-hub install --no-prompt confluentinc/kafka-connect-hdfs3:latest


        # Install ClickHouse Kafka Connect sink
        confluent-hub install --no-prompt clickhouse/kafka-connect-clickhouse:1.0.5

        #

        echo "Launching Kafka Connect worker"

        /etc/confluent/docker/run &

        #

        sleep infinity
    healthcheck:
        test: ["CMD", "curl", "-f", "http://localhost:8083/connectors"]
        interval: 10s
        timeout: 5s
        retries: 5
  
  flink-jobmanager:
    build:
      context: .
      dockerfile: flink-image/dockerfile
    image: my-flink-app:latest
    hostname: jobmanager
    container_name: jobmanager 
    ports:
      - "8000:8081"
    environment:
      - JOB_MANAGER_RPC_ADDRESS=jobmanager
      # - JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
      # - PATH=/usr/lib/jvm/java-11-openjdk-amd64/bin:$PATH
    command: jobmanager
    volumes:
      - ./flink-job:/opt/flink/job
      - ./lib-flink:/opt/flink/lib
  
  flink-taskmanager:
    build: 
      context: .
      dockerfile: flink-image/dockerfile
    image: my-flink-app:latest
    hostname: taskmanager 
    scale: 3
    environment:
      - JOB_MANAGER_RPC_ADDRESS=jobmanager
      # - JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64
      # - PATH=/usr/lib/jvm/java-11-openjdk-amd64/bin:$PATH
    depends_on:
      - flink-jobmanager
    command: taskmanager
    volumes:
      - ./flink-job:/opt/flink/job
      - ./lib-flink:/opt/flink/lib
  
  namenode: 
    image: apache/hadoop:3
    hostname: namenode 
    container_name: namenode
    command: ["hdfs", "namenode"]
    ports:
      - 9870:9870
      - 8020:8020
    # volumes:
    #   - hadoop_namenode_new3:/opt/hadoop/hadoop-root/dfs/name
    env_file:
      - ./config
    environment:
      - ENSURE_NAMENODE_DIR="/opt/hadoop/hadoop-root/dfs/name"
    

  datanode:
    container_name: datanode
    hostname: datanode
    image: apache/hadoop:3
    command: ["hdfs", "datanode"]
    depends_on:
      - namenode 
    env_file:
      - ./config 
    # volumes:
    #   - hadoop_datanode_new3:/opt/hadoop/hadoop-root/dfs/data
  
  resourcemanager:
    image: apache/hadoop:3
    hostname: resourcemanager
    container_name: resourcemanager
    command: ["yarn", "resourcemanager"]
    ports:
      - 8088:8088
    env_file:
      - ./config

  nodemanager:
    image: apache/hadoop:3
    hostname: nodemanager
    container_name: nodemanager
    command: ["yarn", "nodemanager"]
    env_file:
      - ./config 

  
  spark-master:
    # image: apache/spark:3.5.6
    build: 
      context: .
      dockerfile: spark-image/dockerfile
    container_name: spark-master
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
    ports:
      - 7077:7077
      - 8080:8080
      - 6066:6066       # Spark REST API
    volumes:
      - ./spark-apps:/opt/spark/spark-apps
      - ./jars_spark:/opt/spark/jars
      - ./spark-conf/spark-defaults.conf:/opt/spark/conf/spark-defaults.conf
    environment:
      - SPARK_MASTER_HOST=spark-master
      - SPARK_MASTER_PORT=7077
      - SPARK_MASTER_WEBUI_PORT=8080
      - SPARK_MASTER_REST_PORT=6066

  spark-worker:
    # image: apache/spark:3.5.6
    build: 
      context: .
      dockerfile: spark-image/dockerfile
    container_name: spark-worker
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    # environment:
    #   - SPARK_WORKER_CORES=2
    #   - SPARK_WORKER_MEMORY=1G
    #   - SPARK_DRIVER_MEMORY=1G
    depends_on:
      - spark-master
    ports:
      - 8081:8081
    volumes:
      - ./spark-apps:/opt/spark/spark-apps
      - ./jars_spark:/opt/spark/jars
  
  clickhouse-server:
    image: clickhouse/clickhouse-server:latest
    container_name: clickhouse-server
    environment:
      CLICKHOUSE_USER: default
      CLICKHOUSE_PASSWORD: ""  
      CLICKHOUSE_DEFAULT_ACCESS_MANAGEMENT: 1 
    volumes:
      - /mnt/d/project/datapipeline_sales/data_clickhouse/clickhouse:/var/lib/clickhouse
      - ./config_clickhouse.xml:/etc/clickhouse-server/config.xml

    ports:
      - '8123:8123'
      - '9000:9000'  # port native ClickHouse client
      - '9009:9009'  # port untuk inter-server communication jika dibutuhkan
    ulimits:
      nofile:
        soft: 262144
        hard: 262144

  clickhouse-client:
    image: clickhouse/clickhouse-client:latest
    container_name: clickhouse-client
    entrypoint:
      - /bin/sleep
    command:
      - infinity

  webserver:
    <<: *airflow-common
    command: webserver
    container_name: webserver
    ports:
      - "8082:8080"
    depends_on:
      - scheduler

  scheduler:
    <<: *airflow-common
    command: bash -c "airflow db init && \
      airflow users create \
        --username admin \
        --password admin \
        --firstname Raffi \
        --lastname Afif \
        --role Admin \
        --email airscholar@gmail.com && \
      airflow scheduler"

  # spark-master:
  #   <<: *spark-common
  #   command: bin/spark-class org.apache.spark.deploy.master.Master
  #   hostname: spark-master
  #   container_name: spark-master
  #   ports:
  #     - "9090:8080"
  #     - "7077:7077"

  # spark-worker:
  #   <<: *spark-common
  #   hostname: spark-worker
  #   container_name: spark-worker
  #   command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
  #   depends_on:
  #     - spark-master
  #   environment:
  #     SPARK_MODE: worker
  #     SPARK_WORKER_CORES: 2
  #     SPARK_WORKER_MEMORY: 1g
  #     SPARK_MASTER_URL: spark://spark-master:7077
    



volumes:
  sqlserver_data:
  mongodb_data:
  hadoop_namenode_new:
  hadoop_datanode_new:
  hadoop_namenode_new3:
  hadoop_datanode_new3:
  clickhouse-data:



    
      
  